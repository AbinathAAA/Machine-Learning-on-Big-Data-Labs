{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEKwwdFoL8GuPenudgyORb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbinathAAA/Machine-Learning-on-Big-Data-Labs/blob/main/titanic_synthetic_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfW6npam3WXL",
        "outputId": "4b2c81d1-175e-460d-bb93-da1144a1f194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "By5lrBzu3xls"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLUeev-t4G71",
        "outputId": "455cc0ef-f0bc-4207-d539-446f1612dce9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#raw_data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(r\".\\\\diabetes.csv\")\n",
        "dataset = spark.read.csv('drive/My Drive/Colab Notebooks/titanic_synthetic_data.csv',inferSchema=True, header =True)"
      ],
      "metadata": {
        "id": "7Z04oPUR4Ny7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_5nlA7A4Nny",
        "outputId": "2e1d4994-9657-4137-bbb6-c1c4521d3d6d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Pclass',\n",
              " 'Age',\n",
              " 'SibSp',\n",
              " 'Parch',\n",
              " 'Fare',\n",
              " 'Sex',\n",
              " 'Survived',\n",
              " 'Embarked_Q',\n",
              " 'Embarked_S']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.describe().select(\"Summary\",\"Pclass\",\"Age\",\"SibSp\",\"Parch\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKA5xMhl4pML",
        "outputId": "878d5eea-dbf0-4533-e185-e9544ffd9177"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+\n",
            "|Summary|            Pclass|               Age|             SibSp|             Parch|\n",
            "+-------+------------------+------------------+------------------+------------------+\n",
            "|  count|           1000000|           1000000|           1000000|           1000000|\n",
            "|   mean|          1.999411|         39.517847|          4.500906|          4.497493|\n",
            "| stddev|0.8168263709530482|23.092708389398037|2.8714427426552285|2.8728101169393594|\n",
            "|    min|                 1|                 0|                 0|                 0|\n",
            "|    max|                 3|                79|                 9|                 9|\n",
            "+-------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.describe().select(\"Summary\",\"Fare\",\"Sex\",\"Survived\",\"Embarked_Q\",\"Embarked_S\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJoH9pNg6Vna",
        "outputId": "eddf7f7e-7e1b-4fb2-af1b-876d10993df1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+------------------+-------------------+------------------+\n",
            "|Summary|              Fare|               Sex|          Survived|         Embarked_Q|        Embarked_S|\n",
            "+-------+------------------+------------------+------------------+-------------------+------------------+\n",
            "|  count|           1000000|           1000000|           1000000|            1000000|           1000000|\n",
            "|   mean| 254.9829083944402|           0.50082|           0.49987|           0.332717|          0.333661|\n",
            "| stddev|141.53963219142216|0.4999995775993916|0.5000002331001768|0.47118639616145436|0.4715204761307361|\n",
            "|    min|10.000191449724019|                 0|                 0|                  0|                 0|\n",
            "|    max| 499.9999338139367|                 1|                 1|                  1|                 1|\n",
            "+-------+------------------+------------------+------------------+-------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "dataset.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dataset.columns]\n",
        "   ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Saip2hZ6uYT",
        "outputId": "cf08aaac-ae16-48b4-a48c-a635d7cf2dd1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+-----+-----+----+---+--------+----------+----------+\n",
            "|Pclass|Age|SibSp|Parch|Fare|Sex|Survived|Embarked_Q|Embarked_S|\n",
            "+------+---+-----+-----+----+---+--------+----------+----------+\n",
            "|     0|  0|    0|    0|   0|  0|       0|         0|         0|\n",
            "+------+---+-----+-----+----+---+--------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replace min value of zeros with Nan as data cleaning processs\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "dataset = dataset.withColumn(\"Pclass\", when(dataset.Pclass == 0, np.nan).otherwise(dataset.Pclass))\n",
        "dataset = dataset.withColumn(\"Age\", when(dataset.Age == 0, np.nan).otherwise(dataset.Age))\n",
        "dataset = dataset.withColumn(\"Fare\", when(dataset.Fare == 0, np.nan).otherwise(dataset.Fare))\n",
        "\n",
        "dataset.select(\"Pclass\",\"Age\",\"Fare\",\"Sex\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bfwUBFS7TaW",
        "outputId": "d6386442-ec70-4472-9f81-0c59d98bd96f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+------------------+---+\n",
            "|Pclass| Age|              Fare|Sex|\n",
            "+------+----+------------------+---+\n",
            "|   3.0|42.0|397.00178367782024|  0|\n",
            "|   1.0|52.0|302.12770032784186|  0|\n",
            "|   3.0|25.0| 427.1058765283985|  1|\n",
            "|   3.0|32.0| 326.8049924602327|  1|\n",
            "|   1.0|40.0|17.718838497771657|  0|\n",
            "+------+----+------------------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check again NAN values\n",
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "dataset.select([count(when(isnan(c) , c)).alias(c) for c in dataset.columns]\n",
        "   ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI3pXRToCLac",
        "outputId": "fdc6f2dc-142f-4777-d72f-347c529cd4c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+-----+----+---+--------+----------+----------+\n",
            "|Pclass|  Age|SibSp|Parch|Fare|Sex|Survived|Embarked_Q|Embarked_S|\n",
            "+------+-----+-----+-----+----+---+--------+----------+----------+\n",
            "|     0|12475|    0|    0|   0|  0|       0|         0|         0|\n",
            "+------+-----+-----+-----+----+---+--------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "# Impute missing values in Pclass, Age, and Fare\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Pclass\", \"Age\", \"Fare\"],\n",
        "    outputCols=[\"Pclass\", \"Age\", \"Fare\"]\n",
        ")\n",
        "\n",
        "model = imputer.fit(dataset)\n",
        "dataset = model.transform(dataset)\n",
        "\n",
        "dataset.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7SFt6RVEa3o",
        "outputId": "5c8b9ad8-ff98-4e4c-a6fe-2cd9abd67cda"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+-----+-----+------------------+---+--------+----------+----------+\n",
            "|Pclass| Age|SibSp|Parch|              Fare|Sex|Survived|Embarked_Q|Embarked_S|\n",
            "+------+----+-----+-----+------------------+---+--------+----------+----------+\n",
            "|   3.0|42.0|    6|    7|397.00178367782024|  0|       0|         0|         1|\n",
            "|   1.0|52.0|    6|    7|302.12770032784186|  0|       0|         1|         0|\n",
            "|   3.0|25.0|    3|    8| 427.1058765283985|  1|       1|         1|         0|\n",
            "|   3.0|32.0|    9|    7| 326.8049924602327|  1|       0|         0|         0|\n",
            "|   1.0|40.0|    0|    5|17.718838497771657|  0|       0|         0|         0|\n",
            "+------+----+-----+-----+------------------+---+--------+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#us combine all the features in one single feature vector.\n",
        "cols=dataset.columns\n",
        "\n",
        "# Let us import the vector assembler\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")\n",
        "# Now let us use the transform method to transform our dataset\n",
        "dataset=assembler.transform(dataset)\n",
        "dataset.select(\"features\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cm_gSnoGq6h",
        "outputId": "adb6c8f1-d00a-47e1-ad0b-5adc4b7b902c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------+\n",
            "|features                                                         |\n",
            "+-----------------------------------------------------------------+\n",
            "|[3.0,42.0,6.0,7.0,397.00178367782024,0.0,0.0,0.0,1.0]            |\n",
            "|[1.0,52.0,6.0,7.0,302.12770032784186,0.0,0.0,1.0,0.0]            |\n",
            "|[3.0,25.0,3.0,8.0,427.1058765283985,1.0,1.0,1.0,0.0]             |\n",
            "|[3.0,32.0,9.0,7.0,326.8049924602327,1.0,0.0,0.0,0.0]             |\n",
            "|(9,[0,1,3,4],[1.0,40.0,5.0,17.718838497771657])                  |\n",
            "|[1.0,20.0,7.0,2.0,402.54904361394784,0.0,0.0,0.0,0.0]            |\n",
            "|[3.0,76.0,4.0,9.0,52.61038014285054,0.0,0.0,1.0,0.0]             |\n",
            "|[2.0,22.0,4.0,7.0,196.48213366632623,1.0,0.0,0.0,0.0]            |\n",
            "|[3.0,78.0,4.0,8.0,388.6259182464271,1.0,0.0,0.0,0.0]             |\n",
            "|[3.0,15.0,1.0,8.0,299.45900279822325,1.0,0.0,1.0,0.0]            |\n",
            "|[3.0,68.0,8.0,7.0,479.98134512112455,1.0,0.0,0.0,1.0]            |\n",
            "|[3.0,3.0,7.0,1.0,93.03405698775256,0.0,1.0,1.0,0.0]              |\n",
            "|[1.0,63.0,4.0,0.0,84.64050143857706,1.0,0.0,0.0,0.0]             |\n",
            "|[3.0,40.01705982127035,9.0,8.0,296.7771124623517,0.0,1.0,0.0,1.0]|\n",
            "|[2.0,62.0,3.0,4.0,171.31211986091583,1.0,0.0,0.0,0.0]            |\n",
            "|[1.0,41.0,2.0,8.0,433.2922166813368,1.0,1.0,0.0,1.0]             |\n",
            "|[2.0,40.0,4.0,2.0,461.6881960149054,1.0,0.0,0.0,1.0]             |\n",
            "|[2.0,64.0,0.0,4.0,421.0585638320799,0.0,0.0,0.0,1.0]             |\n",
            "|[2.0,67.0,3.0,6.0,119.72564489612725,0.0,1.0,0.0,0.0]            |\n",
            "|[2.0,35.0,0.0,1.0,166.84457106809052,1.0,0.0,0.0,0.0]            |\n",
            "+-----------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Standard Sclarizer\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
        "dataset=standardscaler.fit(dataset).transform(dataset)\n",
        "dataset.select(\"features\",\"Scaled_features\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EvjEkp9HIEO",
        "outputId": "449287e7-f668-458f-9329-5fc9328e4fe2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|            features|     Scaled_features|\n",
            "+--------------------+--------------------+\n",
            "|[3.0,42.0,6.0,7.0...|[3.67275115824148...|\n",
            "|[1.0,52.0,6.0,7.0...|[1.22425038608049...|\n",
            "|[3.0,25.0,3.0,8.0...|[3.67275115824148...|\n",
            "|[3.0,32.0,9.0,7.0...|[3.67275115824148...|\n",
            "|(9,[0,1,3,4],[1.0...|(9,[0,1,3,4],[1.2...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train, test split\n",
        "train, test = dataset.randomSplit([0.8, 0.2], seed=12345)"
      ],
      "metadata": {
        "id": "6NLHCf_2jc6I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #imbalance in the dataset, observe the use of Where\n",
        "dataset_size=float(train.select(\"Survived\").count())\n",
        "numPositives=train.select(\"Survived\").where('Survived == 1').count()\n",
        "per_ones=(float(numPositives)/float(dataset_size))*100\n",
        "numNegatives=float(dataset_size-numPositives)\n",
        "print('The number of ones are {}'.format(numPositives))\n",
        "print('Percentage of ones are {}'.format(per_ones))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK-upEDRmuIu",
        "outputId": "efe50a9e-b2ce-48d0-ebc1-123d91b80df7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of ones are 399854\n",
            "Percentage of ones are 49.96651021438461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BalancingRatio= numNegatives/dataset_size\n",
        "print('BalancingRatio = {}'.format(BalancingRatio))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbxX7XRtoY0c",
        "outputId": "599314e4-a293-46c8-f519-7f145cd2f700"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BalancingRatio = 0.5003348978561539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# balance\n",
        "train=train.withColumn(\"classWeights\", when(train.Survived == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
        "train.select(\"classWeights\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P36JwRolodSr",
        "outputId": "0eea0ee0-8e48-4439-e701-f97adf10085f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|      classWeights|\n",
            "+------------------+\n",
            "|0.5003348978561539|\n",
            "|0.4996651021438461|\n",
            "|0.5003348978561539|\n",
            "|0.5003348978561539|\n",
            "|0.4996651021438461|\n",
            "+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Feature selection\n",
        "# Feature selection using chisquareSelector\n",
        "from pyspark.ml.feature import ChiSqSelector\n",
        "css = ChiSqSelector(featuresCol='Scaled_features',outputCol='Aspect',labelCol='Outcome',fpr=0.05)\n",
        "train=css.fit(train).transform(train)\n",
        "test=css.fit(test).transform(test)\n",
        "test.select(\"Aspect\").show(5,truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "1TEOlwsSqtPS",
        "outputId": "830d09a7-5962-45ef-83d9-d4a5d11e9534"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "Outcome does not exist. Available: Pclass, Age, SibSp, Parch, Fare, Sex, Survived, Embarked_Q, Embarked_S, features, Scaled_features, classWeights",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-34-2206706132.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChiSqSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChiSqSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Scaled_features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Aspect'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Outcome'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Aspect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Outcome does not exist. Available: Pclass, Age, SibSp, Parch, Fare, Sex, Survived, Embarked_Q, Embarked_S, features, Scaled_features, classWeights"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Set the correct label column\n",
        "lr = LogisticRegression(\n",
        "    labelCol=\"Survived\",\n",
        "    featuresCol=\"Aspect\",\n",
        "    weightCol=\"classWeights\",  # Optional, only if you're using class weighting\n",
        "    maxIter=10\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = lr.fit(train)\n",
        "\n",
        "# Predictions\n",
        "predict_train = model.transform(train)\n",
        "predict_test = model.transform(test)\n",
        "\n",
        "# Display predictions\n",
        "predict_test.select(\"Survived\", \"prediction\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "rJ_86jO4rOY6",
        "outputId": "c015179d-86c3-4655-833d-63da5539fa66"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "Aspect does not exist. Available: Pclass, Age, SibSp, Parch, Fare, Sex, Survived, Embarked_Q, Embarked_S, features, Scaled_features, classWeights",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-35-861547067.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Aspect does not exist. Available: Pclass, Age, SibSp, Parch, Fare, Sex, Survived, Embarked_Q, Embarked_S, features, Scaled_features, classWeights"
          ]
        }
      ]
    }
  ]
}